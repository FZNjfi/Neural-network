{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            pixel_0      pixel_1       pixel_2       pixel_3       pixel_4  \\\n",
      "count  50000.000000  50000.00000  50000.000000  50000.000000  50000.000000   \n",
      "mean     130.710740    130.14036    131.050440    131.568860    132.184700   \n",
      "std       73.412873     72.44259     72.240546     72.016555     71.714551   \n",
      "min        0.000000      0.00000      0.000000      0.000000      0.000000   \n",
      "25%       71.000000     71.00000     73.000000     73.000000     75.000000   \n",
      "50%      128.000000    127.00000    129.000000    130.000000    130.000000   \n",
      "75%      189.000000    188.00000    188.000000    188.000000    189.000000   \n",
      "max      255.000000    255.00000    255.000000    255.000000    255.000000   \n",
      "\n",
      "            pixel_5       pixel_6       pixel_7       pixel_8      pixel_9  \\\n",
      "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.00000   \n",
      "mean     132.851840    133.371540    133.890920    134.485040    134.93260   \n",
      "std       71.537505     71.353558     71.281237     71.071698     71.03647   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
      "25%       75.000000     76.000000     76.750000     78.000000     78.00000   \n",
      "50%      131.000000    132.000000    132.000000    133.000000    134.00000   \n",
      "75%      190.000000    190.000000    191.000000    191.000000    192.00000   \n",
      "max      255.000000    255.000000    255.000000    255.000000    255.00000   \n",
      "\n",
      "       ...    pixel_3063    pixel_3064    pixel_3065    pixel_3066  \\\n",
      "count  ...  50000.000000  50000.000000  50000.000000  50000.000000   \n",
      "mean   ...    113.716260    113.735320    113.778520    113.813200   \n",
      "std    ...     64.345623     64.431996     64.401897     64.502684   \n",
      "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
      "25%    ...     64.000000     64.000000     64.000000     64.000000   \n",
      "50%    ...    106.000000    106.000000    106.000000    106.000000   \n",
      "75%    ...    157.000000    157.000000    157.000000    157.000000   \n",
      "max    ...    255.000000    255.000000    255.000000    255.000000   \n",
      "\n",
      "         pixel_3067    pixel_3068    pixel_3069    pixel_3070    pixel_3071  \\\n",
      "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000   \n",
      "mean     113.864120    113.877800    113.830580    113.906240    114.381860   \n",
      "std       64.511269     64.738943     64.894603     65.212671     66.077526   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%       64.000000     64.000000     64.000000     64.000000     63.000000   \n",
      "50%      106.000000    106.000000    106.000000    106.000000    106.000000   \n",
      "75%      157.000000    157.000000    157.000000    157.000000    158.000000   \n",
      "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
      "\n",
      "             label  \n",
      "count  50000.00000  \n",
      "mean       4.50000  \n",
      "std        2.87231  \n",
      "min        0.00000  \n",
      "25%        2.00000  \n",
      "50%        4.50000  \n",
      "75%        7.00000  \n",
      "max        9.00000  \n",
      "\n",
      "[8 rows x 3073 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        data = pd.read_csv(\"train.csv\")\n",
    "        print(data.describe(),\"\\n\")\n",
    "except Exception as e:\n",
    "        print(\"Error! data not  found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        6\n",
       "1        9\n",
       "2        9\n",
       "3        4\n",
       "4        1\n",
       "        ..\n",
       "49995    2\n",
       "49996    6\n",
       "49997    9\n",
       "49998    1\n",
       "49999    1\n",
       "Name: label, Length: 50000, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop(columns=['label'])\n",
    "y = data['label']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_missing_values():\n",
    "    missing = X.isnull().sum()\n",
    "    missing = missing[missing > 0]\n",
    "    if not missing.empty:\n",
    "        print(missing)\n",
    "    else:\n",
    "        print(\"\\n No Missing Data \\n\")\n",
    "\n",
    "def detect_outliers(z_thresh=3.29):\n",
    "    for col in X.columns:\n",
    "        if pd.api.types.is_numeric_dtype(X[col]):\n",
    "            z_scores = zscore(X[col])\n",
    "            outliers = (abs(z_scores) > z_thresh)\n",
    "            num_outliers = outliers.sum()\n",
    "            # rint(f\"col '{col}': {num_outliers} outlier found.\")\n",
    "            #filtered_data = data[np.abs(z_scores) <= z_thresh]\n",
    "def normal(X):\n",
    "    return X.reshape(X.shape[0], -1).astype(np.float32) / 255.0\n",
    "\n",
    "def plot_distributions():\n",
    "    for col in X_train.columns:\n",
    "        if pd.api.types.is_numeric_dtype(X_train[col]):\n",
    "            plt.figure(figsize=(6,4))\n",
    "            sns.histplot(X_train[col], kde=True)\n",
    "            plt.title(f\"Distribution of {col}\")\n",
    "            plt.show()\n",
    "        else:\n",
    "          print(\"\\n\",col, \"is not numeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23137255, 0.16862746, 0.19607843, ..., 0.54901963, 0.32941177,\n",
       "        0.28235295],\n",
       "       [0.6039216 , 0.49411765, 0.4117647 , ..., 0.54509807, 0.5568628 ,\n",
       "        0.5647059 ],\n",
       "       [1.        , 0.99215686, 0.99215686, ..., 0.3254902 , 0.3254902 ,\n",
       "        0.32941177],\n",
       "       ...,\n",
       "       [0.13725491, 0.15686275, 0.16470589, ..., 0.3019608 , 0.25882354,\n",
       "        0.19607843],\n",
       "       [0.7411765 , 0.7294118 , 0.7254902 , ..., 0.6627451 , 0.67058825,\n",
       "        0.67058825],\n",
       "       [0.8980392 , 0.9254902 , 0.91764706, ..., 0.6784314 , 0.63529414,\n",
       "        0.6313726 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.to_numpy().reshape(X.shape[0], -1)\n",
    "y = y.to_numpy()\n",
    "X = normal(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    one_hot[np.arange(y.shape[0]), y] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = one_hot_encode(y_train, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13333334 0.14509805 0.17254902 ... 0.19215687 0.1882353  0.22745098]\n",
      " [0.35686275 0.37254903 0.36078432 ... 0.2901961  0.4745098  0.5254902 ]\n",
      " [0.8156863  0.7764706  0.6431373  ... 0.6392157  0.64705884 0.6156863 ]\n",
      " ...\n",
      " [0.41960785 0.49803922 0.45882353 ... 0.13333334 0.13333334 0.13333334]\n",
      " [1.         1.         1.         ... 0.21960784 0.23137255 0.23921569]\n",
      " [0.73333335 0.77254903 0.78431374 ... 0.37254903 0.40392157 0.41568628]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(\n",
    "    X,y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_activation_function(z):\n",
    "        return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-12\n",
    "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    if len(y_pred.shape) == 1:\n",
    "        y_pred = y_pred.reshape(-1, 1)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def forward(X, W1, b1, W2, b2):\n",
    "    \n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(z2)\n",
    "    # A2 = A2.reshape(-1, 1)\n",
    "    \n",
    "    return z1, A1, z2, A2\n",
    "    \n",
    "def compute_loss(y_true, y_pred):\n",
    "        return binary_cross_entropy(y, y_pred)\n",
    "\n",
    "def backward(X, y, Z1, A1, Z2, A2, W1, W2, lr):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "\n",
    "    dZ2 = A2 - y\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "    \n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * A1 * (1 - A1) \n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "\n",
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, lr):\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "def params(input_dim, hidden_dim = 64, output_dim=10):\n",
    "    W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2./input_dim)\n",
    "    b1 = np.zeros((1, hidden_dim))\n",
    "    \n",
    "    W2 = np.random.randn(hidden_dim, output_dim) * np.sqrt(1./hidden_dim)\n",
    "    b2 = np.zeros((1, output_dim))\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True)) \n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def train(X, y, X_test, y_test, epochs=100, lr=0.1, hidden_dim=64, output_dim=10):\n",
    "    input_dim = X_train.shape[1]\n",
    "    W1, b1, W2, b2 = params(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'test_loss': [],\n",
    "        'accuracy': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass (train)\n",
    "        Z1, A1, Z2, A2 = forward(X_train, W1, b1, W2, b2)\n",
    "        train_loss = binary_cross_entropy(y_train, A2)\n",
    "        \n",
    "        # Backward pass\n",
    "        dW1, db1, dW2, db2 = backward(X_train, y_train, Z1, A1, Z2, A2, W1, W2, lr)\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, lr)\n",
    "        # # Evaluation on test set\n",
    "        # #y_pred_test = forward(X_test, y_test, W, b)\n",
    "        # test_loss = 0# binary_cross_entropy(y_test, y_pred_test)\n",
    "        # accuracy = 0#np.mean((y_pred_test >= 0.5).astype(int) == y_test)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch:3d} | Train Loss: {train_loss}\")\n",
    "    \n",
    "    return W1, b1, W2, b2, history\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X_test, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward(X_test, W1, b1, W2, b2)\n",
    "    return (A2 >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-8)\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    from sklearn.metrics import classification_report\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    print(classification_report(y_true_labels, y_pred_labels))\n",
    "    print(f\"Confusion Matrix:\\nTP: {TP}, FP: {FP}\\nFN: {FN}, TN: {TN}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 2.5058762812585544\n",
      "Epoch  10 | Train Loss: 2.270420476906492\n",
      "Epoch  20 | Train Loss: 2.2393321032965927\n",
      "Epoch  30 | Train Loss: 2.21004840216813\n",
      "Epoch  40 | Train Loss: 2.1831077795212885\n",
      "Epoch  50 | Train Loss: 2.1579739047478714\n",
      "Epoch  60 | Train Loss: 2.1344086658516033\n",
      "Epoch  70 | Train Loss: 2.112360665677736\n",
      "Epoch  80 | Train Loss: 2.0918379674820713\n",
      "Epoch  90 | Train Loss: 2.072846694604499\n",
      "Epoch 100 | Train Loss: 2.0553567305547067\n",
      "Epoch 110 | Train Loss: 2.039297119385927\n",
      "Epoch 120 | Train Loss: 2.0245671633841127\n",
      "Epoch 130 | Train Loss: 2.011050827964946\n",
      "Epoch 140 | Train Loss: 1.998628515778542\n",
      "Epoch 150 | Train Loss: 1.9871849845507976\n",
      "Epoch 160 | Train Loss: 1.97661397397537\n",
      "Epoch 170 | Train Loss: 1.966820352751582\n",
      "Epoch 180 | Train Loss: 1.9577205261318658\n",
      "Epoch 190 | Train Loss: 1.9492417759103549\n",
      "Epoch 200 | Train Loss: 1.9413210760638908\n",
      "Epoch 210 | Train Loss: 1.933903748282267\n",
      "Epoch 220 | Train Loss: 1.9269421595588068\n",
      "Epoch 230 | Train Loss: 1.9203945530417978\n",
      "Epoch 240 | Train Loss: 1.914224041208103\n",
      "Epoch 250 | Train Loss: 1.9083977625457207\n",
      "Epoch 260 | Train Loss: 1.9028861945171867\n",
      "Epoch 270 | Train Loss: 1.89766261389742\n",
      "Epoch 280 | Train Loss: 1.892702691782954\n",
      "Epoch 290 | Train Loss: 1.88798420259085\n",
      "Epoch 300 | Train Loss: 1.8834868186060798\n",
      "Epoch 310 | Train Loss: 1.8791919596144893\n",
      "Epoch 320 | Train Loss: 1.8750826721428075\n",
      "Epoch 330 | Train Loss: 1.8711435215692909\n",
      "Epoch 340 | Train Loss: 1.8673604886709116\n",
      "Epoch 350 | Train Loss: 1.8637208677616741\n",
      "Epoch 360 | Train Loss: 1.8602131663677581\n",
      "Epoch 370 | Train Loss: 1.856827007331941\n",
      "Epoch 380 | Train Loss: 1.8535530343124988\n",
      "Epoch 390 | Train Loss: 1.8503828214391769\n",
      "Epoch 400 | Train Loss: 1.8473087876682512\n",
      "Epoch 410 | Train Loss: 1.844324116205632\n",
      "Epoch 420 | Train Loss: 1.841422679234116\n",
      "Epoch 430 | Train Loss: 1.8385989680689556\n",
      "Epoch 440 | Train Loss: 1.8358480287636987\n",
      "Epoch 450 | Train Loss: 1.8331654030939961\n",
      "Epoch 460 | Train Loss: 1.8305470747640047\n",
      "Epoch 470 | Train Loss: 1.8279894206120912\n",
      "Epoch 480 | Train Loss: 1.8254891665421376\n",
      "Epoch 490 | Train Loss: 1.8230433478740222\n",
      "Epoch 500 | Train Loss: 1.8206492737899926\n",
      "Epoch 510 | Train Loss: 1.818304495549799\n",
      "Epoch 520 | Train Loss: 1.8160067781533544\n",
      "Epoch 530 | Train Loss: 1.8137540751422603\n",
      "Epoch 540 | Train Loss: 1.8115445062482225\n",
      "Epoch 550 | Train Loss: 1.8093763376152057\n",
      "Epoch 560 | Train Loss: 1.807247964341797\n",
      "Epoch 570 | Train Loss: 1.8051578951097207\n",
      "Epoch 580 | Train Loss: 1.8031047386832884\n",
      "Epoch 590 | Train Loss: 1.801087192082507\n",
      "Epoch 600 | Train Loss: 1.799104030249445\n",
      "Epoch 610 | Train Loss: 1.7971540970433113\n",
      "Epoch 620 | Train Loss: 1.7952362974145009\n",
      "Epoch 630 | Train Loss: 1.7933495906216925\n",
      "Epoch 640 | Train Loss: 1.7914929843689358\n",
      "Epoch 650 | Train Loss: 1.7896655297516466\n",
      "Epoch 660 | Train Loss: 1.7878663169115043\n",
      "Epoch 670 | Train Loss: 1.7860944713105091\n",
      "Epoch 680 | Train Loss: 1.784349150543909\n",
      "Epoch 690 | Train Loss: 1.7826295416203903\n",
      "Epoch 700 | Train Loss: 1.7809348586458782\n",
      "Epoch 710 | Train Loss: 1.7792643408545714\n",
      "Epoch 720 | Train Loss: 1.7776172509374133\n",
      "Epoch 730 | Train Loss: 1.7759928736242152\n",
      "Epoch 740 | Train Loss: 1.77439051448105\n",
      "Epoch 750 | Train Loss: 1.7728094988893954\n",
      "Epoch 760 | Train Loss: 1.7712491711779061\n",
      "Epoch 770 | Train Loss: 1.769708893881564\n",
      "Epoch 780 | Train Loss: 1.7681880471064735\n",
      "Epoch 790 | Train Loss: 1.766686027981629\n",
      "Epoch 800 | Train Loss: 1.765202250181723\n",
      "Epoch 810 | Train Loss: 1.7637361435074679\n",
      "Epoch 820 | Train Loss: 1.76228715351199\n",
      "Epoch 830 | Train Loss: 1.7608547411637159\n",
      "Epoch 840 | Train Loss: 1.7594383825377444\n",
      "Epoch 850 | Train Loss: 1.758037568529092\n",
      "Epoch 860 | Train Loss: 1.756651804582379\n",
      "Epoch 870 | Train Loss: 1.755280610433545\n",
      "Epoch 880 | Train Loss: 1.7539235198600596\n",
      "Epoch 890 | Train Loss: 1.752580080436813\n",
      "Epoch 900 | Train Loss: 1.7512498532955145\n",
      "Epoch 910 | Train Loss: 1.7499324128859388\n",
      "Epoch 920 | Train Loss: 1.7486273467377977\n",
      "Epoch 930 | Train Loss: 1.747334255222377\n",
      "Epoch 940 | Train Loss: 1.7460527513133777\n",
      "Epoch 950 | Train Loss: 1.744782460346639\n",
      "Epoch 960 | Train Loss: 1.743523019778617\n",
      "Epoch 970 | Train Loss: 1.7422740789436386\n",
      "Epoch 980 | Train Loss: 1.7410352988100848\n",
      "Epoch 990 | Train Loss: 1.7398063517357287\n",
      "Epoch 1000 | Train Loss: 1.738586921222528\n",
      "Epoch 1010 | Train Loss: 1.7373767016712136\n",
      "Epoch 1020 | Train Loss: 1.7361753981360397\n",
      "Epoch 1030 | Train Loss: 1.7349827260800696\n",
      "Epoch 1040 | Train Loss: 1.7337984111313767\n",
      "Epoch 1050 | Train Loss: 1.732622188840518\n",
      "Epoch 1060 | Train Loss: 1.7314538044396341\n",
      "Epoch 1070 | Train Loss: 1.7302930126034837\n",
      "Epoch 1080 | Train Loss: 1.7291395772127112\n",
      "Epoch 1090 | Train Loss: 1.727993271119596\n",
      "Epoch 1100 | Train Loss: 1.7268538759165146\n",
      "Epoch 1110 | Train Loss: 1.7257211817072897\n",
      "Epoch 1120 | Train Loss: 1.7245949868815864\n",
      "Epoch 1130 | Train Loss: 1.7234750978924624\n",
      "Epoch 1140 | Train Loss: 1.7223613290371618\n",
      "Epoch 1150 | Train Loss: 1.7212535022411946\n",
      "Epoch 1160 | Train Loss: 1.7201514468457324\n",
      "Epoch 1170 | Train Loss: 1.7190549993983049\n",
      "Epoch 1180 | Train Loss: 1.717964003446784\n",
      "Epoch 1190 | Train Loss: 1.716878309336596\n",
      "Epoch 1200 | Train Loss: 1.7157977740111083\n",
      "Epoch 1210 | Train Loss: 1.7147222608151038\n",
      "Epoch 1220 | Train Loss: 1.7136516393012708\n",
      "Epoch 1230 | Train Loss: 1.7125857850396073\n",
      "Epoch 1240 | Train Loss: 1.7115245794296425\n",
      "Epoch 1250 | Train Loss: 1.7104679095153856\n",
      "Epoch 1260 | Train Loss: 1.7094156678029082\n",
      "Epoch 1270 | Train Loss: 1.7083677520804645\n",
      "Epoch 1280 | Train Loss: 1.707324065241075\n",
      "Epoch 1290 | Train Loss: 1.7062845151075021\n",
      "Epoch 1300 | Train Loss: 1.7052490142595502\n",
      "Epoch 1310 | Train Loss: 1.7042174798636551\n",
      "Epoch 1320 | Train Loss: 1.7031898335047175\n",
      "Epoch 1330 | Train Loss: 1.7021660010201736\n",
      "Epoch 1340 | Train Loss: 1.7011459123362973\n",
      "Epoch 1350 | Train Loss: 1.700129501306749\n",
      "Epoch 1360 | Train Loss: 1.6991167055534104\n",
      "Epoch 1370 | Train Loss: 1.6981074663095457\n",
      "Epoch 1380 | Train Loss: 1.697101728265356\n",
      "Epoch 1390 | Train Loss: 1.6960994394160156\n",
      "Epoch 1400 | Train Loss: 1.6951005509122683\n",
      "Epoch 1410 | Train Loss: 1.6941050169136957\n",
      "Epoch 1420 | Train Loss: 1.6931127944447726\n",
      "Epoch 1430 | Train Loss: 1.6921238432538288\n",
      "Epoch 1440 | Train Loss: 1.6911381256750373\n",
      "Epoch 1450 | Train Loss: 1.6901556064935686\n",
      "Epoch 1460 | Train Loss: 1.6891762528140302\n",
      "Epoch 1470 | Train Loss: 1.6882000339323255\n",
      "Epoch 1480 | Train Loss: 1.687226921211044\n",
      "Epoch 1490 | Train Loss: 1.6862568879585045\n",
      "Epoch 1500 | Train Loss: 1.6852899093115539\n",
      "Epoch 1510 | Train Loss: 1.6843259621222118\n",
      "Epoch 1520 | Train Loss: 1.683365024848246\n",
      "Epoch 1530 | Train Loss: 1.6824070774477398\n",
      "Epoch 1540 | Train Loss: 1.6814521012777073\n",
      "Epoch 1550 | Train Loss: 1.6805000789967897\n",
      "Epoch 1560 | Train Loss: 1.679550994472046\n",
      "Epoch 1570 | Train Loss: 1.6786048326898542\n",
      "Epoch 1580 | Train Loss: 1.6776615796708931\n",
      "Epoch 1590 | Train Loss: 1.6767212223891863\n",
      "Epoch 1600 | Train Loss: 1.6757837486951612\n",
      "Epoch 1610 | Train Loss: 1.674849147242663\n",
      "Epoch 1620 | Train Loss: 1.6739174074198548\n",
      "Epoch 1630 | Train Loss: 1.6729885192839185\n",
      "Epoch 1640 | Train Loss: 1.672062473499465\n",
      "Epoch 1650 | Train Loss: 1.671139261280545\n",
      "Epoch 1660 | Train Loss: 1.6702188743361575\n",
      "Epoch 1670 | Train Loss: 1.6693013048191303\n",
      "Epoch 1680 | Train Loss: 1.668386545278251\n",
      "Epoch 1690 | Train Loss: 1.667474588613522\n",
      "Epoch 1700 | Train Loss: 1.6665654280344069\n",
      "Epoch 1710 | Train Loss: 1.665659057020931\n",
      "Epoch 1720 | Train Loss: 1.6647554692875144\n",
      "Epoch 1730 | Train Loss: 1.6638546587493834\n",
      "Epoch 1740 | Train Loss: 1.6629566194914438\n",
      "Epoch 1750 | Train Loss: 1.6620613457394735\n",
      "Epoch 1760 | Train Loss: 1.6611688318335103\n",
      "Epoch 1770 | Train Loss: 1.6602790722032972\n",
      "Epoch 1780 | Train Loss: 1.6593920613456783\n",
      "Epoch 1790 | Train Loss: 1.6585077938038009\n",
      "Epoch 1800 | Train Loss: 1.6576262641480286\n",
      "Epoch 1810 | Train Loss: 1.6567474669584408\n",
      "Epoch 1820 | Train Loss: 1.6558713968088108\n",
      "Epoch 1830 | Train Loss: 1.6549980482519655\n",
      "Epoch 1840 | Train Loss: 1.6541274158064196\n",
      "Epoch 1850 | Train Loss: 1.6532594939441987\n",
      "Epoch 1860 | Train Loss: 1.6523942770797577\n",
      "Epoch 1870 | Train Loss: 1.6515317595599106\n",
      "Epoch 1880 | Train Loss: 1.650671935654695\n",
      "Epoch 1890 | Train Loss: 1.6498147995490982\n",
      "Epoch 1900 | Train Loss: 1.648960345335571\n",
      "Epoch 1910 | Train Loss: 1.6481085670072713\n",
      "Epoch 1920 | Train Loss: 1.6472594584519702\n",
      "Epoch 1930 | Train Loss: 1.6464130134465729\n",
      "Epoch 1940 | Train Loss: 1.645569225652199\n",
      "Epoch 1950 | Train Loss: 1.6447280886097728\n",
      "Epoch 1960 | Train Loss: 1.6438895957360853\n",
      "Epoch 1970 | Train Loss: 1.6430537403202863\n",
      "Epoch 1980 | Train Loss: 1.642220515520771\n",
      "Epoch 1990 | Train Loss: 1.6413899143624275\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, h= train(X_train, y_train, X_test, y_test, 2000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(X_test, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.91      0.19       973\n",
      "           1       0.73      0.25      0.37       979\n",
      "           2       0.75      0.00      0.01      1030\n",
      "           3       0.57      0.01      0.02      1023\n",
      "           4       0.64      0.03      0.05       933\n",
      "           5       0.74      0.06      0.11      1015\n",
      "           6       0.59      0.11      0.19       996\n",
      "           7       0.70      0.17      0.28       994\n",
      "           8       0.74      0.29      0.42      1017\n",
      "           9       0.68      0.21      0.32      1040\n",
      "\n",
      "    accuracy                           0.20     10000\n",
      "   macro avg       0.63      0.20      0.20     10000\n",
      "weighted avg       0.63      0.20      0.20     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "TP: 1339, FP: 564\n",
      "FN: 8661, TN: 89436\n",
      "F1-score: 0.2250\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(h['train_loss'], label='Train Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}