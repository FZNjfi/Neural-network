{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            pixel_0      pixel_1       pixel_2       pixel_3       pixel_4  \\\n",
      "count  50000.000000  50000.00000  50000.000000  50000.000000  50000.000000   \n",
      "mean     130.710740    130.14036    131.050440    131.568860    132.184700   \n",
      "std       73.412873     72.44259     72.240546     72.016555     71.714551   \n",
      "min        0.000000      0.00000      0.000000      0.000000      0.000000   \n",
      "25%       71.000000     71.00000     73.000000     73.000000     75.000000   \n",
      "50%      128.000000    127.00000    129.000000    130.000000    130.000000   \n",
      "75%      189.000000    188.00000    188.000000    188.000000    189.000000   \n",
      "max      255.000000    255.00000    255.000000    255.000000    255.000000   \n",
      "\n",
      "            pixel_5       pixel_6       pixel_7       pixel_8      pixel_9  \\\n",
      "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.00000   \n",
      "mean     132.851840    133.371540    133.890920    134.485040    134.93260   \n",
      "std       71.537505     71.353558     71.281237     71.071698     71.03647   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
      "25%       75.000000     76.000000     76.750000     78.000000     78.00000   \n",
      "50%      131.000000    132.000000    132.000000    133.000000    134.00000   \n",
      "75%      190.000000    190.000000    191.000000    191.000000    192.00000   \n",
      "max      255.000000    255.000000    255.000000    255.000000    255.00000   \n",
      "\n",
      "       ...    pixel_3063    pixel_3064    pixel_3065    pixel_3066  \\\n",
      "count  ...  50000.000000  50000.000000  50000.000000  50000.000000   \n",
      "mean   ...    113.716260    113.735320    113.778520    113.813200   \n",
      "std    ...     64.345623     64.431996     64.401897     64.502684   \n",
      "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
      "25%    ...     64.000000     64.000000     64.000000     64.000000   \n",
      "50%    ...    106.000000    106.000000    106.000000    106.000000   \n",
      "75%    ...    157.000000    157.000000    157.000000    157.000000   \n",
      "max    ...    255.000000    255.000000    255.000000    255.000000   \n",
      "\n",
      "         pixel_3067    pixel_3068    pixel_3069    pixel_3070    pixel_3071  \\\n",
      "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000   \n",
      "mean     113.864120    113.877800    113.830580    113.906240    114.381860   \n",
      "std       64.511269     64.738943     64.894603     65.212671     66.077526   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%       64.000000     64.000000     64.000000     64.000000     63.000000   \n",
      "50%      106.000000    106.000000    106.000000    106.000000    106.000000   \n",
      "75%      157.000000    157.000000    157.000000    157.000000    158.000000   \n",
      "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
      "\n",
      "             label  \n",
      "count  50000.00000  \n",
      "mean       4.50000  \n",
      "std        2.87231  \n",
      "min        0.00000  \n",
      "25%        2.00000  \n",
      "50%        4.50000  \n",
      "75%        7.00000  \n",
      "max        9.00000  \n",
      "\n",
      "[8 rows x 3073 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        data = pd.read_csv(\"train.csv\")\n",
    "        print(data.describe(),\"\\n\")\n",
    "except Exception as e:\n",
    "        print(\"Error! data not  found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "        test = pd.read_csv(\"test.csv\")\n",
    "        print(test.describe(),\"\\n\")\n",
    "except Exception as e:\n",
    "        print(\"Error! data not  found.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = data.drop(columns=['label'])\n",
    "y = data['label']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def check_missing_values():\n",
    "    missing = X.isnull().sum()\n",
    "    missing = missing[missing > 0]\n",
    "    if not missing.empty:\n",
    "        print(missing)\n",
    "    else:\n",
    "        print(\"\\n No Missing Data \\n\")\n",
    "\n",
    "def detect_outliers(z_thresh=3.29):\n",
    "    for col in X.columns:\n",
    "        if pd.api.types.is_numeric_dtype(X[col]):\n",
    "            z_scores = zscore(X[col])\n",
    "            outliers = (abs(z_scores) > z_thresh)\n",
    "            num_outliers = outliers.sum()\n",
    "            print(f\"col '{col}': {num_outliers} outlier found.\")\n",
    "            #filtered_data = data[np.abs(z_scores) <= z_thresh]\n",
    "def normal(X):\n",
    "    return X.reshape(X.shape[0], -1).astype(np.float32) / 255.0\n",
    "\n",
    "def plot_distributions():\n",
    "    for col in X_train.columns:\n",
    "        if pd.api.types.is_numeric_dtype(X_train[col]):\n",
    "            plt.figure(figsize=(6,4))\n",
    "            sns.histplot(X_train[col], kde=True)\n",
    "            plt.title(f\"Distribution of {col}\")\n",
    "            plt.show()\n",
    "        else:\n",
    "          print(\"\\n\",col, \"is not numeric\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "check_missing_values()\n",
    "detect_outliers()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y= np.where(data['label'] == '0', '1', '0')\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = X.to_numpy().reshape(X.shape[0], -1)\n",
    "X = normal(X)\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(\n",
    "    X,y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "print(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = y.to_numpy()  \n",
    "\n",
    "\n",
    "#X = X.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "# plt.imshow(X[0])\n",
    "# plt.title(f'Label: {y[0]}')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = np.array(y).astype(np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sigmoid_activation_function(z):\n",
    "        return sigmoid(z) * (1 - sigmoid(z))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-8 \n",
    "    bce = -np.mean(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
    "    return bce\n",
    "\n",
    "# bce_loss = binary_cross_entropy(y_true, y_pred)\n",
    "# print(f\"Binary Cross-Entropy Loss (manual calculation): {bce_loss}\")\n",
    "\n",
    "# # Compute Binary Cross-Entropy using Keras\n",
    "# bce_loss_keras = binary_crossentropy(y_true, y_pred).numpy()\n",
    "# print(f\"Binary Cross-Entropy Loss (Keras): {bce_loss_keras}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating the Feed forward neural network\n",
    "def f_forward(x, w1, w2):\n",
    "\t# hidden\n",
    "\tz1 = x.dot(w1)    # input from layer 1 \n",
    "\ta1 = sigmoid(z1)  # out put of layer 2 \n",
    "\tz2 = a1.dot(w2)   # input of out layer\n",
    "\ta2 = sigmoid(z2)  # output of out layer\n",
    "\treturn(a2)\n",
    "\n",
    "# initializing the weights randomly\n",
    "def generate_wt(x, y):\n",
    "\tli =[]\n",
    "\tfor i in range(x * y):\n",
    "\t\tli.append(np.random.randn())\n",
    "\treturn(np.array(li).reshape(x, y))\n",
    "\t\n",
    "# Back propagation of error \n",
    "def back_prop(x, y, w1, w2, alpha):\n",
    "\t\n",
    "\t# hidden layer\n",
    "\tz1 = x.dot(w1)\n",
    "\ta1 = sigmoid(z1) \n",
    "\tz2 = a1.dot(w2)\n",
    "\ta2 = sigmoid(z2)\n",
    "\t\n",
    "\t# error in output layer\n",
    "\td2 =(a2-y)\n",
    "\td1 = np.multiply((w2.dot((d2.transpose()))).transpose(), \n",
    "\t\t\t\t\t\t\t\t(np.multiply(a1, 1-a1)))\n",
    "\t# Gradient for w1 and w2\n",
    "\tw1_adj = x.transpose().dot(d1)\n",
    "\tw2_adj = a1.transpose().dot(d2)\n",
    "\t\n",
    "\t# Updating parameters\n",
    "\tw1 = w1-(alpha*(w1_adj))\n",
    "\tw2 = w2-(alpha*(w2_adj))\n",
    "\t\n",
    "\treturn(w1, w2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def forward(X, y, W, b):\n",
    "    z = np.dot(X,W)+ b\n",
    "    y_pred = sigmoid(z)\n",
    "    loss = binary_cross_entropy(y, y_pred)\n",
    "    return z, loss, y_pred\n",
    "    \n",
    "def backward(X, y, W, b, lr, loss, y_pred):\n",
    "    dz = y_pred - y\n",
    "    dw = np.dot(X.T,dz)/X.shape[0]\n",
    "    db= np.mean(dz)\n",
    "\n",
    "    W  -= lr * dw\n",
    "    b -= lr * db\n",
    "\n",
    "    return W, b\n",
    "def params(X):\n",
    "    W = np.random.normal(0, 1/np.sqrt(X.shape[1]), size=X.shape[1])\n",
    "    b = 0\n",
    "    return W,b\n",
    "\n",
    "def train(X, y, epochs=100, lr=0.1):\n",
    "    W , b = params(X)\n",
    "    last_loss=0\n",
    "    loss=10\n",
    "    for epoch in range(epochs):\n",
    "        last_loss = loss\n",
    "        z, loss, ypred =forward(X, y, W, b)\n",
    "        W , b = backward(X, y, W, b, lr, loss, ypred)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} - Loss: {loss:.4f}\")\n",
    "        # if (last_loss-loss)<0.0000001:\n",
    "        #     break\n",
    "    return W , b\n",
    "        \n",
    "         "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(X, W, b):\n",
    "    z = np.dot(X, W) + b\n",
    "    y_pred = sigmoid(z)\n",
    "    return (y_pred >= 0.5).astype(int)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-8)\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "    f1_score = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(f\"TP: {TP}, FP: {FP}\")\n",
    "    print(f\"FN: {FN}, TN: {TN}\")\n",
    "    print(f\"F1-score: {f1_score}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "W , b= train(X_train, y_train, 50, 0.01)\n",
    "y_pred = predict(X_test, W, b)\n",
    "evaluate(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation_function(z):\n",
    "        return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-8 \n",
    "    bce = -np.mean(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
    "    return bce\n",
    "\n",
    "# bce_loss = binary_cross_entropy(y_true, y_pred)\n",
    "# print(f\"Binary Cross-Entropy Loss (manual calculation): {bce_loss}\")\n",
    "\n",
    "# # Compute Binary Cross-Entropy using Keras\n",
    "# bce_loss_keras = binary_crossentropy(y_true, y_pred).numpy()\n",
    "# print(f\"Binary Cross-Entropy Loss (Keras): {bce_loss_keras}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Feed forward neural network\n",
    "def f_forward(x, w1, w2):\n",
    "\t# hidden\n",
    "\tz1 = x.dot(w1)    # input from layer 1 \n",
    "\ta1 = sigmoid(z1)  # out put of layer 2 \n",
    "\tz2 = a1.dot(w2)   # input of out layer\n",
    "\ta2 = sigmoid(z2)  # output of out layer\n",
    "\treturn(a2)\n",
    "\n",
    "# initializing the weights randomly\n",
    "def generate_wt(x, y):\n",
    "\tli =[]\n",
    "\tfor i in range(x * y):\n",
    "\t\tli.append(np.random.randn())\n",
    "\treturn(np.array(li).reshape(x, y))\n",
    "\t\n",
    "# Back propagation of error \n",
    "def back_prop(x, y, w1, w2, alpha):\n",
    "\t\n",
    "\t# hidden layer\n",
    "\tz1 = x.dot(w1)\n",
    "\ta1 = sigmoid(z1) \n",
    "\tz2 = a1.dot(w2)\n",
    "\ta2 = sigmoid(z2)\n",
    "\t\n",
    "\t# error in output layer\n",
    "\td2 =(a2-y)\n",
    "\td1 = np.multiply((w2.dot((d2.transpose()))).transpose(), \n",
    "\t\t\t\t\t\t\t\t(np.multiply(a1, 1-a1)))\n",
    "\t# Gradient for w1 and w2\n",
    "\tw1_adj = x.transpose().dot(d1)\n",
    "\tw2_adj = a1.transpose().dot(d2)\n",
    "\t\n",
    "\t# Updating parameters\n",
    "\tw1 = w1-(alpha*(w1_adj))\n",
    "\tw2 = w2-(alpha*(w2_adj))\n",
    "\t\n",
    "\treturn(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, y, W, b):\n",
    "    z = np.dot(X,W)+ b\n",
    "    y_pred = sigmoid(z)\n",
    "    loss = binary_cross_entropy(y, y_pred)\n",
    "    return z, loss, y_pred\n",
    "    \n",
    "def backward(X, y, W, b, lr, loss, y_pred):\n",
    "    dz = y_pred - y\n",
    "    dw = np.dot(X.T,dz)/X.shape[0]\n",
    "    db= np.mean(dz)\n",
    "\n",
    "    W  -= lr * dw\n",
    "    b -= lr * db\n",
    "\n",
    "    return W, b\n",
    "def params(X):\n",
    "    W = np.random.normal(0, 1/np.sqrt(X.shape[1]), size=X.shape[1])\n",
    "    b = 0\n",
    "    return W,b\n",
    "\n",
    "def train(X, y, epochs=100, lr=0.1):\n",
    "    W , b = params(X)\n",
    "    last_loss=0\n",
    "    loss=10\n",
    "    for epoch in range(epochs):\n",
    "        last_loss = loss\n",
    "        z, loss, ypred =forward(X, y, W, b)\n",
    "        W , b = backward(X, y, W, b, lr, loss, ypred)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} - Loss: {loss:.4f}\")\n",
    "        # if (last_loss-loss)<0.0000001:\n",
    "        #     break\n",
    "    return W , b\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, b):\n",
    "    z = np.dot(X, W) + b\n",
    "    y_pred = sigmoid(z)\n",
    "    return (y_pred >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-8)\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "    f1_score = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(f\"TP: {TP}, FP: {FP}\")\n",
    "    print(f\"FN: {FN}, TN: {TN}\")\n",
    "    print(f\"F1-score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 1.9123\n",
      "Epoch 10 - Loss: -64.3131\n",
      "Epoch 20 - Loss: -64.3135\n",
      "Epoch 30 - Loss: -64.3135\n",
      "Epoch 40 - Loss: -64.3135\n",
      "Confusion Matrix:\n",
      "TP: 979, FP: 973\n",
      "FN: 0, TN: 0\n",
      "F1-score: 0.6680\n"
     ]
    }
   ],
   "source": [
    "W , b= train(X_train, y_train, 50, 0.01)\n",
    "y_pred = predict(X_test, W, b)\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7254902 , 0.7019608 , 0.7372549 , ..., 0.4392157 , 0.4392157 ,\n",
       "        0.44313726],\n",
       "       [0.84313726, 0.827451  , 0.80784315, ..., 0.42352942, 0.42352942,\n",
       "        0.4627451 ],\n",
       "       [0.654902  , 0.6392157 , 0.6392157 , ..., 0.48235294, 0.43529412,\n",
       "        0.43529412],\n",
       "       ...,\n",
       "       [0.5647059 , 0.57254905, 0.5803922 , ..., 0.6117647 , 0.6039216 ,\n",
       "        0.5921569 ],\n",
       "       [0.1764706 , 0.14117648, 0.13725491, ..., 0.16078432, 0.16862746,\n",
       "        0.13725491],\n",
       "       [0.3254902 , 0.3764706 , 0.38039216, ..., 0.34117648, 0.3647059 ,\n",
       "        0.38431373]], dtype=float32)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}